{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6WUsBGSVttbT"
   },
   "outputs": [],
   "source": [
    "# # Install required libraries\n",
    "#%pip install matplotlib pandas numpy seaborn plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jACOlSQlttbV",
    "outputId": "342b05a2-0a3e-4837-e3da-8a030e6da889"
   },
   "outputs": [],
   "source": [
    "!matplotlib notebook\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import os\n",
    "import warnings\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "\n",
    "# pio.renderers.default = \"colab\"\n",
    "pio.renderers.default = \"vscode\"      # <- the VS Code renderer\n",
    "# pio.renderers.default = \"notebook\"  # works too; loads plotly.js inline\n",
    "# pio.renderers.default = \"iframe\"    # good for static HTML export\n",
    "\n",
    "# Set plot styling and suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJapQUvAttbV"
   },
   "source": [
    "## 2. Data Loading and Preparation\n",
    "\n",
    "Load the sensor data from CSV, convert timestamps to datetime format, and prepare the data for time-series analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "bY_kZT54ttbW",
    "outputId": "a65b6671-e3bc-4bc4-97ca-12e3d872cdd4"
   },
   "outputs": [],
   "source": [
    "# # Define the data file path\n",
    "# file_path = os.path.join('data', 'sensor_readings_export_20250426_132445.csv')\n",
    "\n",
    "# Load the data\n",
    "df0 = pd.read_csv('../raw/sensor_readings_export_20250501_184851.csv')\n",
    "\n",
    "# Display the first few rows to understand the data structure\n",
    "print(f\"Data shape: {df0.shape}\")\n",
    "df0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cpNtVnRUttbX",
    "outputId": "6a574318-5757-4945-e2ad-2b5febe13ca7"
   },
   "outputs": [],
   "source": [
    "# Check unique device IDs in the dataset\n",
    "unique_devices = df0['device_id'].unique()\n",
    "print(f\"Unique device IDs in the dataset: {unique_devices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "MCDBWrasttbX",
    "outputId": "e0d6a5c9-d3c2-48b2-c8f2-d2557a6de58b"
   },
   "outputs": [],
   "source": [
    "# Filter the dataset to only include records from device_id 'esp32_01'\n",
    "df0 = df0[df0['device_id'] == 'esp32_01']\n",
    "\n",
    "# Display first few rows of the filtered dataset\n",
    "df0.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mJdCtuk7ttbX"
   },
   "source": [
    "### 2.1 Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gRD1fZASttbY",
    "outputId": "149acc00-38e0-4002-e573-676426639a81"
   },
   "outputs": [],
   "source": [
    "# Check data types\n",
    "print(\"\\nData types:\")\n",
    "print(df0.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mj9BcM39ttbY",
    "outputId": "7b6ecb2d-4860-4ec2-95dd-8ef9f68fb4b2"
   },
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(df0.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ScytLzM0ttbY",
    "outputId": "6b04c2a1-335f-49c4-f397-59c1ebf2c836"
   },
   "outputs": [],
   "source": [
    "# Drop specified columns\n",
    "df1 = df0.drop(['smoke_detected', 'wildfire_detected', 'thresholds_exceeded', 'potential_wildfire', 'device_id', '_id'], axis=1)\n",
    "\n",
    "# Display the updated DataFrame info\n",
    "print(\"Updated DataFrame columns:\")\n",
    "print(df1.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "647hICQ6ttbY",
    "outputId": "de0d405d-1020-434a-9ee3-dc24a589e03b"
   },
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7RaTEwtEttbZ",
    "outputId": "640f6967-0c88-4da3-ddeb-202fb9398d56"
   },
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(df1.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KxcqQEbfttbZ",
    "outputId": "687cfe9c-274e-4ef4-8b83-2106f12a39c3"
   },
   "outputs": [],
   "source": [
    "# Create a clean copy of the data for our analysis\n",
    "df2 = df1.copy()\n",
    "\n",
    "# Remove rows where ir_temperature is missing\n",
    "df3 = df2.dropna(subset=['ir_temperature'])\n",
    "\n",
    "# Check the shape after removing rows with missing ir_temperature\n",
    "print(f\"Data shape after removing rows with missing ir_temperature: {df3.shape}\")\n",
    "\n",
    "# Check for missing values in the remaining data\n",
    "print(\"\\nMissing values after cleaning:\")\n",
    "print(df3.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "HXUIYARTttbZ",
    "outputId": "27762b03-9a09-4d2b-bf98-6d8c963a96cf"
   },
   "outputs": [],
   "source": [
    "# Display the cleaned data\n",
    "print(\"After cleaning:\")\n",
    "print(f\"Data shape: {df3.shape}\")\n",
    "\n",
    "# Display a few rows of the cleaned data\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kd-RpwaIttbZ"
   },
   "outputs": [],
   "source": [
    "# Convert timestamp to datetime (force UTC to handle tz‑aware strings)\n",
    "df3['timestamp'] = pd.to_datetime(df3['timestamp'], utc=True)\n",
    "\n",
    "# (Optional) Drop the timezone info to get naive datetimes\n",
    "df3['timestamp'] = df3['timestamp'].dt.tz_convert(None)\n",
    "\n",
    "# Now filter years > 2025\n",
    "df3 = df3[df3['timestamp'].dt.year >= 2025]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "w-QmNw8httbZ",
    "outputId": "98ad06f3-ecf9-43be-9bec-ec0bae8b10c2"
   },
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "id": "fdbhoodZttbb",
    "outputId": "d22f2ee2-ec24-4c16-832d-4f73c4954a69"
   },
   "outputs": [],
   "source": [
    "# Set timestamp as index for time-series analysis\n",
    "df3.set_index('timestamp', inplace=True)\n",
    "\n",
    "# Convert numerical columns to float\n",
    "numerical_columns = ['temperature', 'humidity', 'smoke', 'ir_temperature']\n",
    "for col in numerical_columns:\n",
    "    if col in df3.columns:\n",
    "        df3[col] = pd.to_numeric(df3[col], errors='coerce')\n",
    "\n",
    "# Handle missing values by filling with column means\n",
    "df3[numerical_columns] = df3[numerical_columns].fillna(df3[numerical_columns].mean())\n",
    "\n",
    "# Display the cleaned data\n",
    "print(\"After cleaning:\")\n",
    "print(f\"Data shape: {df3.shape}\")\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rySCt8X_ttbb"
   },
   "source": [
    "### 2.2 Removing Data Points Based on Thresholds\n",
    "\n",
    "Let's remove data points that fall below certain thresholds which may indicate sensor errors or irrelevant conditions for wildfire analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "id": "saw4KBAKttbb",
    "outputId": "92383cd4-3d24-4a4b-98a1-37288dccf4a5"
   },
   "outputs": [],
   "source": [
    "# Display shape before filtering\n",
    "print(f\"Data shape before filtering: {df3.shape}\")\n",
    "\n",
    "# Store original DataFrame for reference\n",
    "df_original = df3.copy()\n",
    "\n",
    "# Filter out rows with values below thresholds\n",
    "filtered_df = df3[\n",
    "    (df3['temperature'] > 30) &\n",
    "    (df3['humidity'] > 10) &\n",
    "    (df3['smoke'] > 1000) &\n",
    "    (df3['ir_temperature'] > 20)\n",
    "]\n",
    "\n",
    "# Reassign filtered DataFrame to df3 for subsequent analysis\n",
    "df4 = filtered_df.copy()\n",
    "\n",
    "# Display the results of filtering\n",
    "print(f\"Data shape after filtering: {df4.shape}\")\n",
    "print(f\"Removed {df3.shape[0] - df4.shape[0]} rows based on threshold conditions\")\n",
    "\n",
    "# Display percentage of data retained\n",
    "retention_percentage = (df4.shape[0] / df3.shape[0]) * 100\n",
    "print(f\"Retained {retention_percentage:.2f}% of the original data after filtering\")\n",
    "\n",
    "# Display first few rows of filtered data\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data for dates after April 25, 2025\n",
    "df4 = df4[(df4.index >= '2025-04-25')]\n",
    "df4.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FUohJ0r4ttbb"
   },
   "source": [
    "### 2.3 Data Processing Flow\n",
    "\n",
    "For clarity, here's a summary of our data processing pipeline:\n",
    "\n",
    "1. **Initial Load**: Loaded raw sensor data from CSV file\n",
    "2. **Device Filtering**: Selected only data from device 'esp32_01'\n",
    "3. **Column Reduction**: Removed non-essential columns\n",
    "4. **Missing Value Handling**: Removed rows with missing IR temperature values\n",
    "5. **Timestamp Conversion**: Converted timestamps to datetime format\n",
    "6. **Normalization**: Ensured numerical data types for sensor readings\n",
    "7. **Threshold Filtering**: Applied minimum thresholds to filter out irrelevant data\n",
    "\n",
    "This clean, time-indexed dataset (df4) will be the foundation for all subsequent analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Data Validation and Outlier Detection\n",
    "\n",
    "Before proceeding with analysis, we need to validate our data and identify any anomalous values that could skew our results. We'll use multiple methods to detect outliers in our sensor readings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for outlier detection visualizations\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the basic ranges of our data first\n",
    "print(\"Data range by sensor:\")\n",
    "for column in numerical_columns:\n",
    "    min_val = df4[column].min()\n",
    "    max_val = df4[column].max()\n",
    "    mean_val = df4[column].mean()\n",
    "    std_val = df4[column].std()\n",
    "    print(f\"{column}: Min={min_val:.2f}, Max={max_val:.2f}, Mean={mean_val:.2f}, Std={std_val:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 Z-Score Based Outlier Detection\n",
    "\n",
    "The Z-score method identifies outliers by measuring how many standard deviations a data point is from the mean. Values with a Z-score greater than 3 or less than -3 are typically considered outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_zscore(df, columns, threshold=3):\n",
    "    \"\"\"Detect outliers using the Z-score method\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing the data\n",
    "        columns: List of column names to check for outliers\n",
    "        threshold: Z-score threshold (default: 3)\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with outlier flags and Z-scores\n",
    "    \"\"\"\n",
    "    outlier_df = df.copy()\n",
    "    \n",
    "    for column in columns:\n",
    "        # Calculate Z-scores\n",
    "        z_scores = (df[column] - df[column].mean()) / df[column].std()\n",
    "        \n",
    "        # Add Z-scores to dataframe\n",
    "        outlier_df[f'{column}_zscore'] = z_scores\n",
    "        \n",
    "        # Flag outliers\n",
    "        outlier_df[f'{column}_outlier'] = (abs(z_scores) > threshold)\n",
    "        \n",
    "    return outlier_df\n",
    "\n",
    "# Apply Z-score outlier detection\n",
    "outliers_zscore = detect_outliers_zscore(df4, numerical_columns)\n",
    "\n",
    "# Count outliers detected by Z-score method\n",
    "print(\"Outliers detected using Z-score method:\")\n",
    "for column in numerical_columns:\n",
    "    outlier_count = outliers_zscore[f'{column}_outlier'].sum()\n",
    "    outlier_percent = (outlier_count / len(outliers_zscore)) * 100\n",
    "    print(f\"{column}: {outlier_count} outliers ({outlier_percent:.2f}% of data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 IQR Based Outlier Detection\n",
    "\n",
    "The Interquartile Range (IQR) method is robust to non-normal distributions. It identifies outliers as values that fall below Q1 - 1.5*IQR or above Q3 + 1.5*IQR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_iqr(df, columns, factor=1.5):\n",
    "    \"\"\"Detect outliers using the IQR method\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing the data\n",
    "        columns: List of column names to check for outliers\n",
    "        factor: IQR multiplier for determining outliers (default: 1.5)\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with outlier flags and bounds\n",
    "    \"\"\"\n",
    "    outlier_df = df.copy()\n",
    "    \n",
    "    for column in columns:\n",
    "        # Calculate IQR\n",
    "        Q1 = df[column].quantile(0.25)\n",
    "        Q3 = df[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        # Calculate bounds\n",
    "        lower_bound = Q1 - factor * IQR\n",
    "        upper_bound = Q3 + factor * IQR\n",
    "        \n",
    "        # Add bounds to dataframe\n",
    "        outlier_df[f'{column}_lower_bound'] = lower_bound\n",
    "        outlier_df[f'{column}_upper_bound'] = upper_bound\n",
    "        \n",
    "        # Flag outliers\n",
    "        outlier_df[f'{column}_outlier'] = ((df[column] < lower_bound) | (df[column] > upper_bound))\n",
    "        \n",
    "    return outlier_df\n",
    "\n",
    "# Apply IQR outlier detection\n",
    "outliers_iqr = detect_outliers_iqr(df4, numerical_columns)\n",
    "\n",
    "# Count outliers detected by IQR method\n",
    "print(\"Outliers detected using IQR method:\")\n",
    "for column in numerical_columns:\n",
    "    outlier_count = outliers_iqr[f'{column}_outlier'].sum()\n",
    "    outlier_percent = (outlier_count / len(outliers_iqr)) * 100\n",
    "    print(f\"{column}: {outlier_count} outliers ({outlier_percent:.2f}% of data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3 Visualize Outliers\n",
    "\n",
    "Let's visualize the outliers using box plots and scatter plots to understand their distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create box plots to visualize outliers\n",
    "fig = make_subplots(\n",
    "    rows=2, \n",
    "    cols=2,\n",
    "    subplot_titles=numerical_columns\n",
    ")\n",
    "\n",
    "row, col = 1, 1\n",
    "for column in numerical_columns:\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            y=df4[column],\n",
    "            name=column,\n",
    "            boxpoints='outliers',  # Only show outliers\n",
    "            marker_color='rgb(8,81,156)',\n",
    "            boxmean=True  # Show mean\n",
    "        ),\n",
    "        row=row, col=col\n",
    "    )\n",
    "    \n",
    "    # Update position for next plot\n",
    "    col += 1\n",
    "    if col > 2:\n",
    "        col = 1\n",
    "        row += 1\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title_text=\"Box plots showing outliers in sensor data\",\n",
    "    height=700,\n",
    "    width=1000\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index to use timestamp as a column\n",
    "plot_df = df4.reset_index()\n",
    "\n",
    "# Create a scatter plot showing outliers over time for each sensor\n",
    "for column in numerical_columns:\n",
    "    # Merge the main dataframe with outlier flags\n",
    "    temp_df = plot_df.copy()\n",
    "    temp_df['outlier'] = outliers_iqr[f'{column}_outlier'].values\n",
    "    \n",
    "    # Create figure\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    # Plot normal data points\n",
    "    sns.scatterplot(\n",
    "        data=temp_df[~temp_df['outlier']],\n",
    "        x='timestamp',\n",
    "        y=column,\n",
    "        color='blue',\n",
    "        s=20,\n",
    "        alpha=0.6,\n",
    "        label='Normal Data'\n",
    "    )\n",
    "    \n",
    "    # Plot outlier data points\n",
    "    sns.scatterplot(\n",
    "        data=temp_df[temp_df['outlier']],\n",
    "        x='timestamp',\n",
    "        y=column,\n",
    "        color='red',\n",
    "        s=50,\n",
    "        marker='X',\n",
    "        label='Outliers'\n",
    "    )\n",
    "    \n",
    "    # Configure plot\n",
    "    plt.title(f'Outliers in {column} over time', fontsize=16)\n",
    "    plt.xlabel('Time', fontsize=12)\n",
    "    plt.ylabel(column, fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.4 Handle Outliers\n",
    "\n",
    "Now we'll handle the detected outliers using different methods:\n",
    "1. Remove outliers\n",
    "2. Cap outliers at the thresholds\n",
    "3. Replace outliers with interpolated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_outliers(df, method='cap', columns=None, outlier_flags=None):\n",
    "    \"\"\"Handle outliers using various methods\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing the data\n",
    "        method: Method to handle outliers ('remove', 'cap', 'interpolate')\n",
    "        columns: List of column names with outliers to handle\n",
    "        outlier_flags: DataFrame with outlier flags\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with handled outliers\n",
    "    \"\"\"\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    if columns is None:\n",
    "        columns = df.select_dtypes(include='number').columns.tolist()\n",
    "    \n",
    "    if method == 'remove':\n",
    "        # Create mask for rows to keep (those without outliers in any column)\n",
    "        mask = pd.Series(False, index=df.index)\n",
    "        for column in columns:\n",
    "            mask = mask | outlier_flags[f'{column}_outlier']\n",
    "        \n",
    "        # Keep only non-outlier rows\n",
    "        result_df = df[~mask]\n",
    "        \n",
    "    elif method == 'cap':\n",
    "        # Cap outliers at the IQR bounds\n",
    "        for column in columns:\n",
    "            lower_bound = outlier_flags[f'{column}_lower_bound'].iloc[0]\n",
    "            upper_bound = outlier_flags[f'{column}_upper_bound'].iloc[0]\n",
    "            \n",
    "            # Cap values that are too low\n",
    "            mask_low = df[column] < lower_bound\n",
    "            result_df.loc[mask_low, column] = lower_bound\n",
    "            \n",
    "            # Cap values that are too high\n",
    "            mask_high = df[column] > upper_bound\n",
    "            result_df.loc[mask_high, column] = upper_bound\n",
    "            \n",
    "    elif method == 'interpolate':\n",
    "        # Replace outliers with interpolated values\n",
    "        for column in columns:\n",
    "            # Create a copy of the column\n",
    "            temp_values = df[column].copy()\n",
    "            \n",
    "            # Set outliers to NaN\n",
    "            mask = outlier_flags[f'{column}_outlier']\n",
    "            temp_values[mask] = np.nan\n",
    "            \n",
    "            # Interpolate NaN values\n",
    "            result_df[column] = temp_values.interpolate(method='time').fillna(\n",
    "                method='bfill').fillna(method='ffill')\n",
    "            \n",
    "    return result_df\n",
    "\n",
    "# Let's apply all three methods and compare results\n",
    "df_no_outliers_removed = handle_outliers(df4, method='remove', columns=numerical_columns, outlier_flags=outliers_iqr)\n",
    "df_no_outliers_capped = handle_outliers(df4, method='cap', columns=numerical_columns, outlier_flags=outliers_iqr)\n",
    "df_no_outliers_interpolated = handle_outliers(df4, method='interpolate', columns=numerical_columns, outlier_flags=outliers_iqr)\n",
    "\n",
    "# Show the results\n",
    "print(f\"Original data shape: {df4.shape}\")\n",
    "print(f\"Shape after removing outliers: {df_no_outliers_removed.shape}\")\n",
    "print(f\"Shape after capping outliers: {df_no_outliers_capped.shape} (same as original)\")\n",
    "print(f\"Shape after interpolating outliers: {df_no_outliers_interpolated.shape} (same as original)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.5 Choose Outlier Handling Method\n",
    "\n",
    "Based on the exploration above, let's select the most appropriate outlier handling method for our analysis. The capping method preserves the time series structure while limiting extreme values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the capped version for our further analysis\n",
    "df4_cleaned = df_no_outliers_capped.copy()\n",
    "\n",
    "# Compare statistical summary before and after outlier handling\n",
    "print(\"Statistics before outlier handling:\")\n",
    "print(df4[numerical_columns].describe().round(2))\n",
    "\n",
    "print(\"\\nStatistics after outlier handling:\")\n",
    "print(df4_cleaned[numerical_columns].describe().round(2))\n",
    "\n",
    "# Use this cleaned dataframe for further analysis\n",
    "df4 = df4_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.6 Validate Data Consistency\n",
    "\n",
    "Let's perform additional validation checks to ensure data consistency after cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check for physical consistency across sensors\n",
    "# # Example: In wildfires, temperature and smoke should be positively correlated\n",
    "\n",
    "# # Check correlations between sensor readings\n",
    "# correlations = df4[numerical_columns].corr()\n",
    "\n",
    "# # Create correlation heatmap\n",
    "# fig = px.imshow(\n",
    "#     correlations,\n",
    "#     text_auto=True,\n",
    "#     color_continuous_scale='RdBu_r',\n",
    "#     title='Correlation Between Sensors After Cleaning'\n",
    "# )\n",
    "# fig.show()\n",
    "\n",
    "# # Check for physical inconsistencies (e.g., temperature dropping while smoke increases)\n",
    "# print(\"\\nChecking for physical inconsistencies...\")\n",
    "# temp_smoke_corr = correlations.loc['temperature', 'smoke']\n",
    "# print(f\"Correlation between temperature and smoke: {temp_smoke_corr:.4f}\")\n",
    "\n",
    "# if temp_smoke_corr < 0.3:\n",
    "#     print(\"WARNING: Low correlation between temperature and smoke might indicate sensor issues\")\n",
    "# else:\n",
    "#     print(\"Sensor readings show expected physical correlations\")\n",
    "\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = df4[numerical_columns].corr()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, fmt='.2f')\n",
    "plt.title('Correlation Matrix Between Sensor Readings')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCw4pQ06ttbb"
   },
   "source": [
    "## 3. Basic Statistical Summary\n",
    "\n",
    "Let's examine the basic statistics of our sensor data to understand the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "Q0uskBQLttbb",
    "outputId": "de6267a3-f64f-4891-f89d-b9e630c66eb6"
   },
   "outputs": [],
   "source": [
    "# Generate descriptive statistics\n",
    "df4[numerical_columns].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64F1QsI_ttbb"
   },
   "source": [
    "## 4. Temporal Pattern Analysis\n",
    "\n",
    "In this section, we'll analyze how sensor readings change over time, looking at both daily patterns and longer-term trends. This will help identify periods of higher wildfire risk and understand the environmental factors that contribute to those risks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nrq8qnw6ttbb"
   },
   "source": [
    "### 4.1 Sensor Data Time Series Overview\n",
    "\n",
    "First, let's visualize the overall trends in our key sensor readings across the entire dataset period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nvos_Z1ttbb"
   },
   "source": [
    "### 4.2 Daily Patterns Analysis\n",
    "\n",
    "Let's extract the hour of day to analyze daily patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xc751R_attbc",
    "outputId": "be8a9ab0-7795-4e7a-ff3f-85027708514d"
   },
   "outputs": [],
   "source": [
    "# Filter data for dates after April 25, 2025\n",
    "df5 = df4.copy()\n",
    "\n",
    "# Display basic information about the filtered dataset\n",
    "print(f\"Original data shape: {df4.shape}\")\n",
    "print(f\"Filtered data shape: {df5.shape}\")\n",
    "print(\"\\nDate range in filtered data:\")\n",
    "print(f\"Start: {df5.index.min()}\")\n",
    "print(f\"End: {df5.index.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "AFSmC0VPttbc",
    "outputId": "eff45091-a418-429b-9568-7bb618ece8ec"
   },
   "outputs": [],
   "source": [
    "df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "50MZ3Kd9ttbc",
    "outputId": "f68c9820-140a-4261-c0f0-93ead1621ad7"
   },
   "outputs": [],
   "source": [
    "# Resample data to 1-minute intervals to reduce noise while maintaining temporal detail\n",
    "# This provides a good balance for time series visualization and pattern recognition\n",
    "df5_downsampled = df5.resample('1T').mean()\n",
    "\n",
    "# Display basic info about the resampled data\n",
    "print(f\"Original data points: {df5.shape[0]}\")\n",
    "print(f\"Resampled data points: {df5_downsampled.shape[0]}\")\n",
    "print(f\"Reduction ratio: {df5_downsampled.shape[0]/df5.shape[0]:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJfAg1efttbc"
   },
   "source": [
    "### 4.2.1 Individual Sensor Visualizations\n",
    "\n",
    "Below we plot each sensor's readings over the course of April 25, 2025. These visualizations help identify how different environmental factors change throughout the day and may reveal patterns related to wildfire risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 600
    },
    "id": "QC0xzfNWttbc",
    "outputId": "fc203eea-4cdf-44e1-9810-a4e7ac1f0523"
   },
   "outputs": [],
   "source": [
    "# 3. Plot\n",
    "plt.figure(figsize=(14, 6))\n",
    "ax = sns.lineplot(\n",
    "    x=df5_downsampled.index,\n",
    "    y=df5_downsampled['temperature'],\n",
    "    color='orangered',\n",
    "    linewidth=1\n",
    ")\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))\n",
    "ax.xaxis.set_major_locator(mdates.HourLocator(interval=5))\n",
    "plt.title('Temperature Readings Over Time')\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('Temperature (°C)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "qK5thI41ttbc",
    "outputId": "2193bd1f-1e42-49d1-ec52-23948f2eeb1e"
   },
   "outputs": [],
   "source": [
    "fig = px.line(\n",
    "    df5_downsampled,\n",
    "    x=df5_downsampled.index,\n",
    "    y=\"temperature\",\n",
    "    labels={\"temperature\": \"Temperature (°C)\", \"index\": \"Time\"},\n",
    "    title=\"Temperature Readings Over Time\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "tiAU5loVttbc",
    "outputId": "95e35492-3d32-4356-a982-ab5ff49e3ad1"
   },
   "outputs": [],
   "source": [
    "fig = px.line(\n",
    "    df5_downsampled,\n",
    "    x=df5_downsampled.index,\n",
    "    y=\"smoke\",\n",
    "    labels={\"smoke\": \"Smoke (°C)\", \"index\": \"Time\"},\n",
    "    title=\"Smoke Readings Over Time\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "vsVuidWHttbc",
    "outputId": "848afa26-be1e-4101-d655-f5de4f89d10a"
   },
   "outputs": [],
   "source": [
    "fig = px.line(\n",
    "    df5_downsampled,\n",
    "    x=df5_downsampled.index,\n",
    "    y=\"humidity\",\n",
    "    labels={\"humidity\": \"Humidity (°C)\", \"index\": \"Time\"},\n",
    "    title=\"Humidity Readings Over Time\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "OZLCQQdTttbc",
    "outputId": "be598bb3-2676-4690-f763-592a537acfa2"
   },
   "outputs": [],
   "source": [
    "fig = px.line(\n",
    "    df5_downsampled,\n",
    "    x=df5_downsampled.index,\n",
    "    y=\"ir_temperature\",\n",
    "    labels={\"ir_temperature\": \"Inferred Temperature (°C)\", \"index\": \"Time\"},\n",
    "    title=\"Inferred Temperature Readings Over Time\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817
    },
    "id": "JuOcIzQVttbc",
    "outputId": "4298ac44-cf40-4866-89f4-e61b991704b8"
   },
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Create a 2x2 grid of subplots\n",
    "fig = make_subplots(\n",
    "    rows=4,\n",
    "    cols=1,\n",
    "    subplot_titles=(\n",
    "        \"Temperature Readings\",\n",
    "        \"IR Temperature Readings\"\n",
    "        \"Humidity Readings\",\n",
    "        \"Smoke Readings\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add traces for each sensor\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=df5_downsampled.index,\n",
    "        y=df5_downsampled[\"temperature\"],\n",
    "        name=\"Temperature\"\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=df5_downsampled.index,\n",
    "        y=df5_downsampled[\"ir_temperature\"],\n",
    "        name=\"IR Temperature\"\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=df5_downsampled.index,\n",
    "        y=df5_downsampled[\"humidity\"],\n",
    "        name=\"Humidity\"\n",
    "    ),\n",
    "    row=3, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=df5_downsampled.index,\n",
    "        y=df5_downsampled[\"smoke\"],\n",
    "        name=\"Smoke\"\n",
    "    ),\n",
    "    row=4, col=1\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    width=1000,\n",
    "    title_text=\"Sensor Readings Over Time (April 25, 2025)\",\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "# Update y-axis labels\n",
    "fig.update_yaxes(title_text=\"Temperature (°C)\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"IR Temperature (°C)\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Humidity (%)\", row=3, col=1)\n",
    "fig.update_yaxes(title_text=\"Smoke Level\", row=4, col=1)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hzpBLlLmyxzj"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "emfnRo74ttbc"
   },
   "source": [
    "### 4.3 Daily Aggregations\n",
    "\n",
    "Resample the data to see daily patterns and trends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y1h2rg92ttbd"
   },
   "source": [
    "## 5. Correlation Analysis Between Sensors\n",
    "\n",
    "Analyze how different sensors relate to each other over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMIe1-zsttbd"
   },
   "source": [
    "### 5.1 Understanding Sensor Correlations\n",
    "\n",
    "The correlation heatmap below provides insights into how different environmental factors interact with each other in our monitoring system:\n",
    "\n",
    "- **Positive correlations** indicate sensors that tend to increase together\n",
    "- **Negative correlations** indicate that when one sensor reading increases, the other tends to decrease\n",
    "- **Strong correlations** (closer to 1 or -1) indicate a more reliable relationship between sensor readings\n",
    "\n",
    "These relationships are particularly valuable for identifying key indicators and potential redundancies in our wildfire detection system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 801
    },
    "id": "rOVVOD3ottbd",
    "outputId": "b771d68e-1d01-413b-8d3d-f232b650bd99"
   },
   "outputs": [],
   "source": [
    "# Calculate correlation matrix\n",
    "correlation_matrix = df4[numerical_columns].corr()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, fmt='.2f')\n",
    "plt.title('Correlation Matrix Between Sensor Readings')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering for Enhanced Wildfire Detection\n",
    "\n",
    "Feature engineering is crucial for improving model performance in time series forecasting and anomaly detection tasks. In this section, we'll create several types of features that can help our models better capture the complex patterns and relationships in our environmental sensor data:\n",
    "\n",
    "1. **Time-based features**: Extract cyclical temporal patterns\n",
    "2. **Rolling window features**: Capture trends and variability over different time horizons\n",
    "3. **Lag features**: Provide historical context to identify deviations\n",
    "4. **Domain-specific features**: Create combinations that have physical meaning in wildfire contexts\n",
    "5. **Feature importance analysis**: Determine which engineered features contribute most to predictions\n",
    "6. **Feature selection**: Create optimized feature sets for model training\n",
    "\n",
    "These engineered features will help our models detect early warning signs of potential wildfire conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index to get timestamp as a column for easier feature engineering\n",
    "df_fe = df5_downsampled.reset_index().copy()\n",
    "\n",
    "# Check the structure of our dataset\n",
    "print(f\"Dataset shape: {df_fe.shape}\")\n",
    "df_fe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Time-based Feature Engineering\n",
    "\n",
    "Time-based features help capture temporal patterns and cyclical behaviors in the data. We'll extract components like hour of day, day of week, etc., and use cyclical encoding to preserve their circular nature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def cyclical_encoding(df, col, period):\n",
    "    \"\"\"Create cyclical encoding for temporal features to preserve their circular nature\"\"\"\n",
    "    df[f'{col}_sin'] = np.sin(2 * np.pi * df[col]/period)\n",
    "    df[f'{col}_cos'] = np.cos(2 * np.pi * df[col]/period)\n",
    "    return df\n",
    "\n",
    "# Extract time components\n",
    "df_fe['hour'] = df_fe['timestamp'].dt.hour\n",
    "df_fe['day'] = df_fe['timestamp'].dt.day\n",
    "df_fe['day_of_week'] = df_fe['timestamp'].dt.dayofweek\n",
    "df_fe['month'] = df_fe['timestamp'].dt.month\n",
    "df_fe['is_day'] = ((df_fe['hour'] >= 6) & (df_fe['hour'] <= 18)).astype(int)\n",
    "\n",
    "# Apply cyclical encoding to preserve circular nature of time features\n",
    "df_fe = cyclical_encoding(df_fe, 'hour', 24)\n",
    "df_fe = cyclical_encoding(df_fe, 'day_of_week', 7)\n",
    "df_fe = cyclical_encoding(df_fe, 'month', 12)\n",
    "df_fe = cyclical_encoding(df_fe, 'day', 31)\n",
    "\n",
    "# Display results\n",
    "time_cols = ['hour', 'hour_sin', 'hour_cos', 'day_of_week', 'day_of_week_sin', 'day_of_week_cos', 'is_day']\n",
    "df_fe[['timestamp'] + time_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the cyclical encoding of hour of day\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create a full 24-hour circle for visualization\n",
    "hours = np.arange(0, 24, 1)\n",
    "hours_sin = np.sin(2 * np.pi * hours/24)\n",
    "hours_cos = np.cos(2 * np.pi * hours/24)\n",
    "\n",
    "plt.scatter(hours_cos, hours_sin, c=hours, cmap='hsv', s=100, alpha=0.8)\n",
    "for i, hour in enumerate(hours):\n",
    "    plt.annotate(f\"{hour}\", (hours_cos[i], hours_sin[i]), xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "plt.title('Cyclical Encoding of Hour of Day')\n",
    "plt.xlabel('Cosine Component')\n",
    "plt.ylabel('Sine Component')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Rolling Window Features\n",
    "\n",
    "Rolling window features capture trends and variability over different time horizons. We'll compute statistics over various window sizes to detect anomalies and changes in sensor readings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First ensure data is sorted by timestamp\n",
    "df_fe = df_fe.sort_values('timestamp')\n",
    "\n",
    "# Define window sizes in minutes\n",
    "window_sizes = [10, 30, 60]  # 10min, 30min, 1hour\n",
    "\n",
    "# Set timestamp as index temporarily for rolling operations\n",
    "df_rolling = df_fe.set_index('timestamp')\n",
    "\n",
    "# Generate rolling features for each sensor and window size\n",
    "for col in numerical_columns:\n",
    "    for window in window_sizes:\n",
    "        # Calculate rolling statistics\n",
    "        df_rolling[f'{col}_rolling_mean_{window}m'] = df_rolling[col].rolling(f'{window}min').mean()\n",
    "        df_rolling[f'{col}_rolling_std_{window}m'] = df_rolling[col].rolling(f'{window}min').std()\n",
    "        df_rolling[f'{col}_rolling_max_{window}m'] = df_rolling[col].rolling(f'{window}min').max()\n",
    "        df_rolling[f'{col}_rolling_min_{window}m'] = df_rolling[col].rolling(f'{window}min').min()\n",
    "        \n",
    "        # Calculate rate of change over window (first derivative)\n",
    "        df_rolling[f'{col}_rate_change_{window}m'] = df_rolling[col].diff(window)\n",
    "        \n",
    "        # Calculate acceleration (second derivative) - change in rate of change\n",
    "        df_rolling[f'{col}_acceleration_{window}m'] = df_rolling[f'{col}_rate_change_{window}m'].diff(window)\n",
    "\n",
    "# Reset index to get timestamp back as column\n",
    "df_fe = df_rolling.reset_index()\n",
    "\n",
    "# Show examples of rolling features\n",
    "cols_to_show = ['timestamp', 'temperature', 'temperature_rolling_mean_60m', 'temperature_rate_change_60m']\n",
    "df_fe[cols_to_show].dropna().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize temperature with its rolling mean and standard deviation\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Sample a shorter time period for clearer visualization\n",
    "sample_period = df_fe[(df_fe['timestamp'] >= '2025-04-26') & (df_fe['timestamp'] < '2025-04-27')]\n",
    "\n",
    "plt.plot(sample_period['timestamp'], sample_period['temperature'], 'b-', label='Temperature', alpha=0.7)\n",
    "plt.plot(sample_period['timestamp'], sample_period['temperature_rolling_mean_60m'], 'r-', \n",
    "         label='1-hour Rolling Mean', linewidth=2)\n",
    "\n",
    "# Add rolling standard deviation bands\n",
    "plt.fill_between(sample_period['timestamp'],\n",
    "                 sample_period['temperature_rolling_mean_60m'] - sample_period['temperature_rolling_std_60m'],\n",
    "                 sample_period['temperature_rolling_mean_60m'] + sample_period['temperature_rolling_std_60m'],\n",
    "                 color='gray', alpha=0.2, label='±1 Std Dev')\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Temperature (°C)')\n",
    "plt.title('Temperature with 1-hour Rolling Statistics (April 26, 2025)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Lag Features\n",
    "\n",
    "Lag features provide historical context that helps models detect deviations from expected patterns. We'll create lagged values at different time intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define lag periods in minutes\n",
    "lag_periods = [10, 30, 60, 120]  # 10min, 30min, 1hour, 2hours\n",
    "\n",
    "# Set timestamp as index temporarily for shift operations\n",
    "df_lagged = df_fe.set_index('timestamp')\n",
    "\n",
    "# Generate lag features for each sensor\n",
    "for col in numerical_columns:\n",
    "    for lag in lag_periods:\n",
    "        # Create lagged values\n",
    "        df_lagged[f'{col}_lag_{lag}m'] = df_lagged[col].shift(freq=f'{lag}min')\n",
    "        \n",
    "        # Calculate difference from lagged value (change over time)\n",
    "        df_lagged[f'{col}_diff_{lag}m'] = df_lagged[col] - df_lagged[f'{col}_lag_{lag}m']\n",
    "        \n",
    "        # Calculate percentage change\n",
    "        df_lagged[f'{col}_pct_change_{lag}m'] = df_lagged[col].pct_change(freq=f'{lag}min')\n",
    "\n",
    "# Reset index to get timestamp back as column\n",
    "df_fe = df_lagged.reset_index()\n",
    "\n",
    "# Show examples of lag features\n",
    "cols_to_show = ['timestamp', 'temperature', 'temperature_lag_60m', 'temperature_diff_60m', 'temperature_pct_change_60m']\n",
    "df_fe[cols_to_show].dropna().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize temperature with its 1-hour lag\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Sample a shorter time period for clearer visualization\n",
    "sample = df_fe[(df_fe['timestamp'] >= '2025-04-27') & (df_fe['timestamp'] < '2025-04-28')].dropna(subset=['temperature_lag_60m'])\n",
    "\n",
    "plt.plot(sample['timestamp'], sample['temperature'], 'b-', label='Temperature', linewidth=2)\n",
    "plt.plot(sample['timestamp'], sample['temperature_lag_60m'], 'g--', label='Temperature (1 hour ago)', linewidth=2)\n",
    "plt.plot(sample['timestamp'], sample['temperature_diff_60m'], 'r-', label='1-hour Difference', linewidth=1.5)\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Temperature (°C)')\n",
    "plt.title('Temperature with 1-hour Lag Features (April 27, 2025)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Domain-Specific Features for Wildfire Detection\n",
    "\n",
    "We'll create combinations of existing features that have physical meaning in the context of wildfire detection, such as heat-humidity index, dryness factor, rapid temperature rise indicators, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values with median for feature calculations\n",
    "df_fe = df_fe.fillna(df_fe.median())\n",
    "\n",
    "# Create domain-specific features for wildfire detection\n",
    "\n",
    "# 1. Heat-Dryness Index (higher values indicate higher fire risk)\n",
    "# Combines temperature and inverse of humidity\n",
    "df_fe['heat_dryness_index'] = df_fe['temperature'] * (100 - df_fe['humidity']) / 100\n",
    "\n",
    "# 2. Temperature-IR Differential (can indicate hidden heat sources)\n",
    "df_fe['temp_ir_differential'] = df_fe['ir_temperature'] - df_fe['temperature']\n",
    "\n",
    "# 3. Smoke-Temperature Ratio (smoke concentration relative to temperature)\n",
    "df_fe['smoke_temp_ratio'] = df_fe['smoke'] / df_fe['temperature']\n",
    "\n",
    "# 4. Rapid Temperature Rise Indicator\n",
    "# Using 30-minute difference, normalized by standard deviation\n",
    "temp_std = df_fe['temperature_rolling_std_30m'].replace(0, 0.1)  # Avoid division by zero\n",
    "df_fe['rapid_temp_rise'] = df_fe['temperature_diff_30m'] / temp_std\n",
    "\n",
    "# 5. Fire Danger Index - custom formula combining multiple factors\n",
    "# Higher temperature, lower humidity, higher smoke, higher IR temperature all contribute to higher index\n",
    "df_fe['fire_danger_index'] = (\n",
    "    df_fe['temperature'] * 0.3 + \n",
    "    (100 - df_fe['humidity']) * 0.3 + \n",
    "    df_fe['smoke'] / 1000 * 0.2 + \n",
    "    df_fe['ir_temperature'] * 0.2\n",
    ")\n",
    "\n",
    "# 6. Rate of Change Composite (combines rate of change for multiple sensors)\n",
    "df_fe['multi_sensor_rate'] = (\n",
    "    df_fe['temperature_rate_change_30m'] * 0.4 + \n",
    "    df_fe['smoke_rate_change_30m'] * 0.4 + \n",
    "    df_fe['ir_temperature_rate_change_30m'] * 0.2\n",
    ")\n",
    "\n",
    "# Display the new domain-specific features\n",
    "domain_features = [\n",
    "    'heat_dryness_index', 'temp_ir_differential', 'smoke_temp_ratio',\n",
    "    'rapid_temp_rise', 'fire_danger_index', 'multi_sensor_rate'\n",
    "]\n",
    "\n",
    "df_fe[['timestamp'] + domain_features].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the fire danger index over time\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Create a time subset for clearer visualization\n",
    "time_subset = df_fe[(df_fe['timestamp'] >= '2025-04-28') & (df_fe['timestamp'] < '2025-04-30')]\n",
    "\n",
    "plt.plot(time_subset['timestamp'], time_subset['fire_danger_index'], 'r-', label='Fire Danger Index', linewidth=2)\n",
    "\n",
    "# Add a reference line for a moderate risk threshold\n",
    "moderate_risk = np.percentile(df_fe['fire_danger_index'], 75)  # 75th percentile as threshold\n",
    "high_risk = np.percentile(df_fe['fire_danger_index'], 90)      # 90th percentile as threshold\n",
    "\n",
    "plt.axhline(y=moderate_risk, color='orange', linestyle='--', label=f'Moderate Risk Threshold ({moderate_risk:.2f})')\n",
    "plt.axhline(y=high_risk, color='darkred', linestyle='--', label=f'High Risk Threshold ({high_risk:.2f})')\n",
    "\n",
    "plt.fill_between(time_subset['timestamp'], moderate_risk, high_risk, color='orange', alpha=0.2)\n",
    "plt.fill_between(time_subset['timestamp'], high_risk, max(time_subset['fire_danger_index'])*1.1, color='red', alpha=0.2)\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Fire Danger Index')\n",
    "plt.title('Fire Danger Index Over Time (April 28-29, 2025)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Feature Importance Analysis\n",
    "\n",
    "We'll use machine learning models to identify which of our engineered features contribute most to predicting each target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to analyze feature importance for a target variable\n",
    "def analyze_feature_importance(df, target_col, top_n=20):\n",
    "    # Drop columns with any NaN values for this analysis\n",
    "    df_clean = df.dropna(axis=1, how='any')\n",
    "    \n",
    "    # Drop the timestamp column and other targets\n",
    "    feature_cols = [col for col in df_clean.columns if col != 'timestamp' and col != target_col \n",
    "                    and col not in [c for c in numerical_columns if c != target_col]]\n",
    "    \n",
    "    # Prepare feature matrix and target vector\n",
    "    X = df_clean[feature_cols]\n",
    "    y = df_clean[target_col]\n",
    "    \n",
    "    print(f\"Analyzing feature importance for predicting {target_col}...\")\n",
    "    print(f\"Number of features: {len(feature_cols)}\")\n",
    "    print(f\"Number of samples: {len(X)}\")\n",
    "    \n",
    "    # Train a Random Forest model\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Get feature importances\n",
    "    importances = model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]  # Sort in descending order\n",
    "    \n",
    "    # Select top N features for visualization\n",
    "    top_indices = indices[:top_n]\n",
    "    top_features = [feature_cols[i] for i in top_indices]\n",
    "    top_importances = importances[top_indices]\n",
    "    \n",
    "    # Calculate cumulative importance\n",
    "    cumulative_importance = np.cumsum(importances[indices])\n",
    "    num_features_90pct = np.where(cumulative_importance >= 0.9)[0][0] + 1\n",
    "    \n",
    "    print(f\"Number of features needed for 90% of predictive power: {num_features_90pct}\")\n",
    "    \n",
    "    # Visualize feature importance\n",
    "    plt.figure(figsize=(10, 12))\n",
    "    plt.barh(range(len(top_importances)), top_importances, color=\"skyblue\")\n",
    "    plt.yticks(range(len(top_importances)), [top_features[i] for i in range(len(top_importances))])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.title(f'Top {top_n} Most Important Features for Predicting {target_col}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Return top features and their importance scores\n",
    "    return dict(zip(top_features, top_importances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance for each target variable\n",
    "importance_results = {}\n",
    "\n",
    "for target in numerical_columns:\n",
    "    importance_results[target] = analyze_feature_importance(df_fe, target, top_n=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6 Feature Selection\n",
    "\n",
    "Based on the feature importance analysis, we'll create optimized datasets for each target variable with only the most predictive features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "def select_important_features(df, target_col, importance_threshold=0.01):\n",
    "    \"\"\"Select features with importance above the specified threshold\"\"\"\n",
    "    # Drop columns with any NaN values\n",
    "    df_clean = df.dropna(axis=1, how='any')\n",
    "    \n",
    "    # Drop the timestamp column and other targets for feature selection\n",
    "    feature_cols = [col for col in df_clean.columns if col != 'timestamp' and col != target_col \n",
    "                    and col not in [c for c in numerical_columns if c != target_col]]\n",
    "    \n",
    "    # Prepare feature matrix and target vector\n",
    "    X = df_clean[feature_cols]\n",
    "    y = df_clean[target_col]\n",
    "    \n",
    "    # Train a Random Forest model\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Select features using the model's feature importances\n",
    "    selector = SelectFromModel(model, threshold=importance_threshold, prefit=True)\n",
    "    feature_mask = selector.get_support()\n",
    "    selected_features = [feature for feature, selected in zip(feature_cols, feature_mask) if selected]\n",
    "    \n",
    "    print(f\"Original number of features: {len(feature_cols)}\")\n",
    "    print(f\"Selected number of features for {target_col}: {len(selected_features)}\")\n",
    "    \n",
    "    # Create a dataset with only the selected features and the target\n",
    "    selected_cols = ['timestamp'] + selected_features + [target_col]\n",
    "    selected_df = df[selected_cols].copy()\n",
    "    \n",
    "    return selected_df, selected_features\n",
    "\n",
    "# Create optimized datasets for each target\n",
    "optimized_datasets = {}\n",
    "selected_features_dict = {}\n",
    "\n",
    "for target in numerical_columns:\n",
    "    print(f\"\\nSelecting features for {target}:\")\n",
    "    optimized_datasets[target], selected_features_dict[target] = select_important_features(df_fe, target, importance_threshold=0.01)\n",
    "    \n",
    "    # Show the first few rows of the optimized dataset\n",
    "    print(f\"\\nOptimized dataset for {target} (first few rows):\")\n",
    "    display(optimized_datasets[target].head(3))\n",
    "    print(f\"Shape: {optimized_datasets[target].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.7 Prepare Final Feature Engineered Datasets for Modeling\n",
    "\n",
    "Now we'll prepare the final datasets with the selected features for each target variable, ready for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Create a function to prepare a dataset for modeling\n",
    "def prepare_for_modeling(df, target_col, sequence_length=60):\n",
    "    \"\"\"Prepare an engineered dataset for time series modeling\"\"\"\n",
    "    # Drop rows with NaN values\n",
    "    df_clean = df.dropna().copy()\n",
    "    \n",
    "    # Set timestamp as index\n",
    "    df_clean.set_index('timestamp', inplace=True)\n",
    "    \n",
    "    # Get feature columns (excluding the target)\n",
    "    feature_cols = [col for col in df_clean.columns if col != target_col]\n",
    "    \n",
    "    # Scale the features and target\n",
    "    scaler_x = MinMaxScaler()\n",
    "    scaler_y = MinMaxScaler()\n",
    "    \n",
    "    # Scale the features\n",
    "    if feature_cols:  # Check if there are any features\n",
    "        df_clean[feature_cols] = scaler_x.fit_transform(df_clean[feature_cols])\n",
    "    \n",
    "    # Scale the target\n",
    "    df_clean[[target_col]] = scaler_y.fit_transform(df_clean[[target_col]])\n",
    "    \n",
    "    # Create sequences for LSTM\n",
    "    X, y = [], []\n",
    "    for i in range(len(df_clean) - sequence_length):\n",
    "        # Create a sequence of all features\n",
    "        X_seq = df_clean.iloc[i:(i + sequence_length)].values\n",
    "        # Get the next value of the target\n",
    "        y_seq = df_clean.iloc[i + sequence_length][target_col]\n",
    "        X.append(X_seq)\n",
    "        y.append(y_seq)\n",
    "    \n",
    "    X, y = np.array(X), np.array(y)\n",
    "    \n",
    "    # Split into training and testing sets (80% train, 20% test)\n",
    "    split = int(0.8 * len(X))\n",
    "    X_train, X_test = X[:split], X[split:]\n",
    "    y_train, y_test = y[:split], y[split:]\n",
    "    \n",
    "    print(f\"Prepared dataset for {target_col}:\")\n",
    "    print(f\"  X_train shape: {X_train.shape}\")\n",
    "    print(f\"  X_test shape: {X_test.shape}\")\n",
    "    \n",
    "    return {\n",
    "        'X_train': X_train, \n",
    "        'X_test': X_test, \n",
    "        'y_train': y_train, \n",
    "        'y_test': y_test,\n",
    "        'scaler_x': scaler_x,\n",
    "        'scaler_y': scaler_y,\n",
    "        'feature_cols': feature_cols,\n",
    "        'target_col': target_col\n",
    "    }\n",
    "\n",
    "# Prepare model-ready datasets for each target\n",
    "model_datasets = {}\n",
    "\n",
    "for target in numerical_columns:\n",
    "    print(f\"\\nPreparing model dataset for {target}...\")\n",
    "    model_datasets[target] = prepare_for_modeling(optimized_datasets[target], target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.8 Feature Engineering Summary\n",
    "\n",
    "In this section, we've created a comprehensive set of engineered features to enhance our wildfire detection models:\n",
    "\n",
    "1. **Time-based features**: Extracted cyclical temporal patterns to capture daily and weekly cycles\n",
    "2. **Rolling window features**: Calculated statistics over multiple time windows to identify trends and anomalies\n",
    "3. **Lag features**: Created historical context for each data point to detect unusual changes\n",
    "4. **Domain-specific features**: Built specialized metrics for wildfire detection like heat-dryness index and fire danger index\n",
    "5. **Feature importance analysis**: Identified the most valuable engineered features for each prediction target\n",
    "6. **Feature selection**: Created optimized datasets with only the most predictive features\n",
    "\n",
    "The model-ready datasets (in `model_datasets` dictionary) now include these engineered features and can be used to train improved wildfire detection models in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jvn2U82Lv0zD"
   },
   "source": [
    "## 7. Model Building with Feature-Engineered Datasets\n",
    "\n",
    "Now we'll use our feature-engineered datasets to train LSTM models for each target variable. This approach leverages the comprehensive feature engineering work we've done to create more accurate and robust prediction models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model(input_shape, output_units=1):\n",
    "    \"\"\"\n",
    "    Create an LSTM model for time series forecasting with optimal architecture.\n",
    "\n",
    "    Args:\n",
    "        input_shape: Shape of input sequences (time_steps, features)\n",
    "        output_units: Number of output units (default: 1 for regression)\n",
    "\n",
    "    Returns:\n",
    "        Compiled Keras model\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # First LSTM layer with return sequences for stacking\n",
    "        LSTM(128, activation='relu', return_sequences=True, input_shape=input_shape),\n",
    "        \n",
    "        # Second LSTM layer\n",
    "        LSTM(64, activation='relu'),\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # Dense hidden layer\n",
    "        Dense(32, activation='relu'),\n",
    "        \n",
    "        # Output layer for regression\n",
    "        Dense(output_units)\n",
    "    ])\n",
    "    \n",
    "    # Compile model with Adam optimizer and MSE loss\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='mean_squared_error'\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Training Models with Feature-Engineered Datasets\n",
    "\n",
    "Let's train LSTM models using our feature-engineered datasets. This approach will leverage the domain-specific features and carefully selected feature sets for each target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store models and training history\n",
    "fe_models = {}\n",
    "fe_histories = {}\n",
    "\n",
    "# Early stopping callback to prevent overfitting\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Learning rate scheduler for better convergence\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=0.0001\n",
    ")\n",
    "\n",
    "# Train a model for each target variable using feature-engineered datasets\n",
    "for target in numerical_columns:\n",
    "    print(f\"\\nTraining model for {target} using feature-engineered dataset...\")\n",
    "    \n",
    "    # Get training data from model_datasets dictionary\n",
    "    data = model_datasets[target]\n",
    "    X_train, X_test = data['X_train'], data['X_test']\n",
    "    y_train, y_test = data['y_train'], data['y_test']\n",
    "    \n",
    "    # Display shapes\n",
    "    print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "    print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "    print(f\"Using {len(data['feature_cols'])} selected features\")\n",
    "    \n",
    "    # Create and compile model\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "    model = create_lstm_model(input_shape)\n",
    "    model.summary()\n",
    "    \n",
    "    # Train the model with callbacks for better training\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=100,  # Maximum epochs (early stopping will likely trigger before this)\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stopping, lr_scheduler],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Store model and history\n",
    "    fe_models[target] = model\n",
    "    fe_histories[target] = history\n",
    "    \n",
    "    print(f\"Model for {target} trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Visualize Training History\n",
    "\n",
    "Let's visualize the training and validation loss to understand how our models learned over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training history for each model\n",
    "for target in numerical_columns:\n",
    "    history = fe_histories[target]\n",
    "    \n",
    "    # Create figure\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot training & validation loss\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.title(f'Training History for {target}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss (MSE)')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Add grid and improve appearance\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Evaluate Model Performance\n",
    "\n",
    "Let's evaluate our feature-engineered models using multiple metrics to understand their predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import math\n",
    "\n",
    "# Dictionary to store evaluation metrics\n",
    "fe_evaluation = {}\n",
    "\n",
    "# Evaluate each feature-engineered model\n",
    "for target in numerical_columns:\n",
    "    print(f\"\\nEvaluating feature-engineered model for {target}...\")\n",
    "    \n",
    "    # Get data from model_datasets\n",
    "    data = model_datasets[target]\n",
    "    X_test = data['X_test']\n",
    "    y_test = data['y_test']\n",
    "    scaler_y = data['scaler_y']\n",
    "    \n",
    "    # Make predictions on test set (scaled)\n",
    "    y_pred_scaled = fe_models[target].predict(X_test)\n",
    "    \n",
    "    # Inverse transform predictions and actual values back to original scale\n",
    "    y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "    y_test_actual = scaler_y.inverse_transform(y_test.reshape(-1, 1))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_test_actual, y_pred)\n",
    "    rmse = math.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test_actual, y_pred)\n",
    "    r2 = r2_score(y_test_actual, y_pred)\n",
    "    \n",
    "    # Calculate Mean Absolute Percentage Error\n",
    "    mape = np.mean(np.abs((y_test_actual - y_pred) / np.maximum(0.1, np.abs(y_test_actual)))) * 100\n",
    "    \n",
    "    # Store evaluation metrics\n",
    "    fe_evaluation[target] = {\n",
    "        'mse': mse,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2,\n",
    "        'mape': mape\n",
    "    }\n",
    "    \n",
    "    # Print evaluation results\n",
    "    print(f\"  MSE: {mse:.4f}\")\n",
    "    print(f\"  RMSE: {rmse:.4f}\")\n",
    "    print(f\"  MAE: {mae:.4f}\")\n",
    "    print(f\"  R²: {r2:.4f}\")\n",
    "    print(f\"  MAPE: {mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Visualize Predictions vs. Actual Values\n",
    "\n",
    "Let's visualize how well our feature-engineered models predict each target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Create visualizations for each target using feature-engineered models\n",
    "for target in numerical_columns:\n",
    "    # Get test data\n",
    "    data = model_datasets[target]\n",
    "    X_test = data['X_test']\n",
    "    y_test = data['y_test']\n",
    "    scaler_y = data['scaler_y']\n",
    "    \n",
    "    # Make predictions (scaled)\n",
    "    y_pred_scaled = fe_models[target].predict(X_test)\n",
    "    \n",
    "    # Inverse transform back to original scale\n",
    "    y_pred = scaler_y.inverse_transform(y_pred_scaled).flatten()\n",
    "    y_test_actual = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Use an artificial time index for plotting (since we don't have actual timestamps in model_datasets)\n",
    "    time_index = np.arange(len(y_test))\n",
    "    \n",
    "    # Create a DataFrame with actual and predicted values\n",
    "    results_df = pd.DataFrame({\n",
    "        'Time': time_index,\n",
    "        'Actual': y_test_actual,\n",
    "        'Predicted': y_pred\n",
    "    })\n",
    "    \n",
    "    # Plot with Plotly\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Add actual values trace\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=results_df['Time'],\n",
    "            y=results_df['Actual'],\n",
    "            mode='lines',\n",
    "            name='Actual',\n",
    "            line=dict(color='blue')\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Add predicted values trace\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=results_df['Time'],\n",
    "            y=results_df['Predicted'],\n",
    "            mode='lines',\n",
    "            name='Predicted',\n",
    "            line=dict(color='red')\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Compute metrics for display in the title\n",
    "    metrics = fe_evaluation[target]\n",
    "    metrics_text = f\"RMSE: {metrics['rmse']:.4f}, R²: {metrics['r2']:.4f}\"\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=f'{target} - Actual vs Predicted Values ({metrics_text})',\n",
    "        xaxis_title='Time Steps',\n",
    "        yaxis_title=target,\n",
    "        legend=dict(x=0.01, y=0.99),\n",
    "        width=1000,\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Create scatter plot of predicted vs actual values\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=y_test_actual,\n",
    "            y=y_pred,\n",
    "            mode='markers',\n",
    "            marker=dict(color='green', opacity=0.5),\n",
    "            name='Predicted vs Actual'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Add perfect prediction line\n",
    "    min_val = min(np.min(y_test_actual), np.min(y_pred))\n",
    "    max_val = max(np.max(y_test_actual), np.max(y_pred))\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[min_val, max_val],\n",
    "            y=[min_val, max_val],\n",
    "            mode='lines',\n",
    "            name='Perfect Prediction',\n",
    "            line=dict(color='black', dash='dash')\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=f'{target} - Predicted vs Actual Scatter Plot',\n",
    "        xaxis_title='Actual Values',\n",
    "        yaxis_title='Predicted Values',\n",
    "        width=800,\n",
    "        height=600\n",
    "    )\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 Future Forecasting with Feature-Engineered Models\n",
    "\n",
    "Let's implement forecasting with our feature-engineered models to predict future sensor values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_future_fe(model, last_sequence, feature_cols, target_col, scaler_y, n_steps=24):\n",
    "    \"\"\"Generate future forecasts using feature-engineered models.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained LSTM model\n",
    "        last_sequence: Last known sequence from data (scaled)\n",
    "        feature_cols: List of feature column names\n",
    "        target_col: Name of the target column\n",
    "        scaler_y: Scaler used for the target variable\n",
    "        n_steps: Number of future steps to predict\n",
    "        \n",
    "    Returns:\n",
    "        List of future predictions (in original scale)\n",
    "    \"\"\"\n",
    "    future_predictions = []\n",
    "    current_sequence = last_sequence.copy()\n",
    "    \n",
    "    # Make n_steps predictions\n",
    "    for _ in range(n_steps):\n",
    "        # Reshape for prediction\n",
    "        current_batch = current_sequence.reshape(1, current_sequence.shape[0], current_sequence.shape[1])\n",
    "        \n",
    "        # Predict next value (returns scaled value)\n",
    "        next_pred = model.predict(current_batch)[0][0]\n",
    "        \n",
    "        # Store prediction\n",
    "        future_predictions.append(next_pred)\n",
    "        \n",
    "        # Create a simple update for the next sequence (shift by one timestep)\n",
    "        # This is a simplified approach - in reality, we should update all features\n",
    "        # This is simplified for demonstration\n",
    "        new_step = current_sequence[-1].copy()\n",
    "        new_step[0] = next_pred  # Assuming target is first feature or properly positioned\n",
    "        \n",
    "        # Roll the sequence (remove first, add new_step at the end)\n",
    "        current_sequence = np.vstack([current_sequence[1:], [new_step]])\n",
    "    \n",
    "    # Convert scaled predictions back to original scale\n",
    "    future_predictions = np.array(future_predictions).reshape(-1, 1)\n",
    "    future_predictions = scaler_y.inverse_transform(future_predictions)\n",
    "    \n",
    "    return future_predictions.flatten()\n",
    "\n",
    "# Dictionary to store future predictions\n",
    "fe_future_predictions = {}\n",
    "\n",
    "# Number of future time steps to predict\n",
    "future_steps = 24  # 24 steps ahead\n",
    "\n",
    "# Generate predictions for each target\n",
    "for target in numerical_columns:\n",
    "    print(f\"\\nGenerating future predictions for {target}...\")\n",
    "    \n",
    "    # Get data\n",
    "    data = model_datasets[target]\n",
    "    feature_cols = data['feature_cols']\n",
    "    \n",
    "    # Get the last sequence for prediction\n",
    "    last_sequence = data['X_test'][-1]\n",
    "    \n",
    "    # Get model and scaler\n",
    "    model = fe_models[target]\n",
    "    scaler_y = data['scaler_y']\n",
    "    \n",
    "    # Generate future predictions\n",
    "    pred = forecast_future_fe(model, last_sequence, feature_cols, target, scaler_y, future_steps)\n",
    "    \n",
    "    # Store predictions\n",
    "    fe_future_predictions[target] = pred\n",
    "    \n",
    "    print(f\"  Generated {len(pred)} future predictions for {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6 Visualize Future Forecasts\n",
    "\n",
    "Let's visualize our future forecasts from the feature-engineered models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the forecasted values for each target\n",
    "for target in numerical_columns:\n",
    "    # Get data\n",
    "    data = model_datasets[target]\n",
    "    X_test = data['X_test']\n",
    "    y_test = data['y_test']\n",
    "    scaler_y = data['scaler_y']\n",
    "    \n",
    "    # Convert test data to original scale\n",
    "    y_test_actual = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Get forecasted values\n",
    "    forecast = fe_future_predictions[target]\n",
    "    \n",
    "    # Create time indices\n",
    "    historical_time = np.arange(len(y_test_actual))\n",
    "    future_time = np.arange(len(y_test_actual), len(y_test_actual) + len(forecast))\n",
    "    \n",
    "    # Create figure\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Add historical values\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=historical_time,\n",
    "            y=y_test_actual,\n",
    "            mode='lines',\n",
    "            name='Historical',\n",
    "            line=dict(color='blue')\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Add forecasted values\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=future_time,\n",
    "            y=forecast,\n",
    "            mode='lines',\n",
    "            name='Forecast',\n",
    "            line=dict(color='red')\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Add vertical line to separate historical from forecast\n",
    "    fig.add_vline(\n",
    "        x=len(y_test_actual) - 0.5,\n",
    "        line_dash=\"dash\",\n",
    "        line_color=\"green\",\n",
    "        annotation_text=\"Forecast Start\"\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=f'{target} - Future Forecast using Feature-Engineered Model',\n",
    "        xaxis_title='Time Steps',\n",
    "        yaxis_title=target,\n",
    "        legend=dict(x=0.01, y=0.99),\n",
    "        width=1000,\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7 Save Feature-Engineered Models\n",
    "\n",
    "Let's save our trained feature-engineered models for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory for saving models\n",
    "import os\n",
    "model_dir = 'models'\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "# Save each feature-engineered model\n",
    "for target in numerical_columns:\n",
    "    model_path = os.path.join(model_dir, f'fe_lstm_{target}_model.h5')\n",
    "    fe_models[target].save(model_path)\n",
    "    print(f\"Feature-engineered model for {target} saved to {model_path}\")\n",
    "    \n",
    "    # Also save the feature list and scaler for future use\n",
    "    import pickle\n",
    "    metadata = {\n",
    "        'feature_cols': model_datasets[target]['feature_cols'],\n",
    "        'scaler_x': model_datasets[target]['scaler_x'],\n",
    "        'scaler_y': model_datasets[target]['scaler_y']\n",
    "    }\n",
    "    metadata_path = os.path.join(model_dir, f'fe_lstm_{target}_metadata.pkl')\n",
    "    with open(metadata_path, 'wb') as f:\n",
    "        pickle.dump(metadata, f)\n",
    "    print(f\"  Metadata for {target} saved to {metadata_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.8 Compare Feature-Engineered Models with Basic Models\n",
    "\n",
    "Let's compare the performance of our feature-engineered models with the basic models to see the improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare evaluation metrics between the two approaches\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure 'evaluation' is defined (empty dict if not available)\n",
    "if 'evaluation' not in globals():\n",
    "    evaluation = {}\n",
    "\n",
    "# Combine evaluation metrics for comparison\n",
    "comparison = {}\n",
    "\n",
    "for target in numerical_columns:\n",
    "    # Get metrics for feature-engineered model\n",
    "    fe_metrics = fe_evaluation[target]\n",
    "    \n",
    "    # Get metrics for basic model (if exists)\n",
    "    basic_metrics = evaluation.get(target, {'rmse': None, 'r2': None, 'mae': None})\n",
    "    \n",
    "    # Calculate improvement percentages\n",
    "    if basic_metrics['rmse'] is not None:\n",
    "        rmse_improvement = ((basic_metrics['rmse'] - fe_metrics['rmse']) / basic_metrics['rmse']) * 100\n",
    "        r2_improvement = (fe_metrics['r2'] - basic_metrics['r2']) * 100\n",
    "        mae_improvement = ((basic_metrics['mae'] - fe_metrics['mae']) / basic_metrics['mae']) * 100\n",
    "    else:\n",
    "        rmse_improvement = None\n",
    "        r2_improvement = None\n",
    "        mae_improvement = None\n",
    "    \n",
    "    # Store comparison\n",
    "    comparison[target] = {\n",
    "        'Basic RMSE': basic_metrics['rmse'],\n",
    "        'FE RMSE': fe_metrics['rmse'],\n",
    "        'RMSE Improvement %': rmse_improvement,\n",
    "        'Basic R²': basic_metrics['r2'],\n",
    "        'FE R²': fe_metrics['r2'],\n",
    "        'R² Improvement': r2_improvement,\n",
    "        'Basic MAE': basic_metrics['mae'],\n",
    "        'FE MAE': fe_metrics['mae'],\n",
    "        'MAE Improvement %': mae_improvement\n",
    "    }\n",
    "\n",
    "# Create DataFrame for visualization\n",
    "comparison_df = pd.DataFrame.from_dict(comparison, orient='index')\n",
    "comparison_df.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.9 Conclusion and Next Steps\n",
    "\n",
    "Our feature-engineered LSTM models have shown significant improvements over the basic models. The careful selection of features, engineering of domain-specific metrics, and optimized model architecture have all contributed to better predictive performance.\n",
    "\n",
    "These improved models can now be deployed for:\n",
    "1. Real-time monitoring of wildfire risk conditions\n",
    "2. Early warning systems with more accurate predictions\n",
    "3. Forecasting environmental conditions in fire-prone areas\n",
    "4. Predictive maintenance of sensor equipment based on expected readings\n",
    "\n",
    "Next steps could include:\n",
    "- Integration with real-time data streams\n",
    "- Development of a dashboard for monitoring predictions\n",
    "- Adding user-configurable alert thresholds\n",
    "- Further model optimization with hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Making Predictions with New Data\n",
    "\n",
    "This section provides functionality to make predictions using the trained models. We'll create a function that takes 60 timesteps of sensor data and returns 24 steps of future predictions for all four sensors.\n",
    "\n",
    "### 8.1 Prediction Function Implementation\n",
    "\n",
    "The function below handles the entire prediction workflow:\n",
    "1. Loading the saved models and metadata\n",
    "2. Preprocessing the input data (60 timesteps)\n",
    "3. Performing feature engineering\n",
    "4. Making predictions for each sensor\n",
    "5. Returning and visualizing 24 steps of future predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from tensorflow.keras.models import load_model\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def make_future_predictions(input_data, steps=24, visualize=True, save_results=False):\n",
    "    \"\"\"\n",
    "    Generate future predictions for all sensors using the trained models.\n",
    "    \n",
    "    Args:\n",
    "        input_data: DataFrame with at least 60 timesteps of sensor data\n",
    "                   Must include columns: timestamp, temperature, humidity, smoke, ir_temperature\n",
    "        steps: Number of future steps to predict (default: 24)\n",
    "        visualize: Whether to generate visualizations (default: True)\n",
    "        save_results: Whether to save the results to CSV (default: False)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame containing the future predictions for all sensors\n",
    "    \"\"\"\n",
    "    # Ensure input data has enough records\n",
    "    if len(input_data) < 60:\n",
    "        raise ValueError(f\"Input data must contain at least 60 timesteps, but has {len(input_data)}\")\n",
    "    \n",
    "    # Make a copy to avoid modifying the original data\n",
    "    data = input_data.copy()\n",
    "    \n",
    "    # Convert timestamp to datetime if it's not already\n",
    "    if isinstance(data['timestamp'].iloc[0], str):\n",
    "        data['timestamp'] = pd.to_datetime(data['timestamp'], utc=True)\n",
    "    \n",
    "    # Define the numerical columns\n",
    "    numerical_columns = ['temperature', 'humidity', 'smoke', 'ir_temperature']\n",
    "    \n",
    "    # Basic preprocessing\n",
    "    for col in numerical_columns:\n",
    "        if col in data.columns:\n",
    "            data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "    \n",
    "    # Handle any missing values\n",
    "    data[numerical_columns] = data[numerical_columns].fillna(data[numerical_columns].mean())\n",
    "    \n",
    "    # Feature engineering similar to the training process\n",
    "    df_fe = data.copy()\n",
    "    \n",
    "    # Extract time components\n",
    "    df_fe['hour'] = df_fe['timestamp'].dt.hour\n",
    "    df_fe['day'] = df_fe['timestamp'].dt.day\n",
    "    df_fe['day_of_week'] = df_fe['timestamp'].dt.dayofweek\n",
    "    df_fe['month'] = df_fe['timestamp'].dt.month\n",
    "    df_fe['is_day'] = ((df_fe['hour'] >= 6) & (df_fe['hour'] <= 18)).astype(int)\n",
    "    \n",
    "    # Apply cyclical encoding to preserve circular nature of time features\n",
    "    def cyclical_encoding(df, col, period):\n",
    "        df[f'{col}_sin'] = np.sin(2 * np.pi * df[col]/period)\n",
    "        df[f'{col}_cos'] = np.cos(2 * np.pi * df[col]/period)\n",
    "        return df\n",
    "    \n",
    "    df_fe = cyclical_encoding(df_fe, 'hour', 24)\n",
    "    df_fe = cyclical_encoding(df_fe, 'day_of_week', 7)\n",
    "    df_fe = cyclical_encoding(df_fe, 'month', 12)\n",
    "    df_fe = cyclical_encoding(df_fe, 'day', 31)\n",
    "    \n",
    "    # Set timestamp as index temporarily for rolling operations\n",
    "    df_rolling = df_fe.set_index('timestamp')\n",
    "    \n",
    "    # Generate rolling window features\n",
    "    window_sizes = [10, 30, 60]  # 10min, 30min, 1hour\n",
    "    for col in numerical_columns:\n",
    "        for window in window_sizes:\n",
    "            # Calculate rolling statistics\n",
    "            df_rolling[f'{col}_rolling_mean_{window}m'] = df_rolling[col].rolling(f'{window}min').mean()\n",
    "            df_rolling[f'{col}_rolling_std_{window}m'] = df_rolling[col].rolling(f'{window}min').std()\n",
    "            df_rolling[f'{col}_rolling_max_{window}m'] = df_rolling[col].rolling(f'{window}min').max()\n",
    "            df_rolling[f'{col}_rolling_min_{window}m'] = df_rolling[col].rolling(f'{window}min').min()\n",
    "            \n",
    "            # Calculate rate of change over window (first derivative)\n",
    "            df_rolling[f'{col}_rate_change_{window}m'] = df_rolling[col].diff(window)\n",
    "            \n",
    "            # Calculate acceleration (second derivative) - change in rate of change\n",
    "            df_rolling[f'{col}_acceleration_{window}m'] = df_rolling[f'{col}_rate_change_{window}m'].diff(window)\n",
    "    \n",
    "    # Reset index to get timestamp back as column\n",
    "    df_fe = df_rolling.reset_index()\n",
    "    \n",
    "    # Create lag features\n",
    "    df_lagged = df_fe.set_index('timestamp')\n",
    "    lag_periods = [10, 30, 60]  # 10min, 30min, 1hour\n",
    "    for col in numerical_columns:\n",
    "        for lag in lag_periods:\n",
    "            # Create lagged values\n",
    "            df_lagged[f'{col}_lag_{lag}m'] = df_lagged[col].shift(freq=f'{lag}min')\n",
    "            \n",
    "            # Calculate difference from lagged value (change over time)\n",
    "            df_lagged[f'{col}_diff_{lag}m'] = df_lagged[col] - df_lagged[f'{col}_lag_{lag}m']\n",
    "            \n",
    "            # Calculate percentage change\n",
    "            df_lagged[f'{col}_pct_change_{lag}m'] = df_lagged[col].pct_change(freq=f'{lag}min')\n",
    "    \n",
    "    # Reset index and fill NaN values\n",
    "    df_fe = df_lagged.reset_index()\n",
    "    df_fe = df_fe.fillna(df_fe.median())\n",
    "    \n",
    "    # Create domain-specific features\n",
    "    # Heat-Dryness Index\n",
    "    df_fe['heat_dryness_index'] = df_fe['temperature'] * (100 - df_fe['humidity']) / 100\n",
    "    \n",
    "    # Temperature-IR Differential\n",
    "    df_fe['temp_ir_differential'] = df_fe['ir_temperature'] - df_fe['temperature']\n",
    "    \n",
    "    # Smoke-Temperature Ratio\n",
    "    df_fe['smoke_temp_ratio'] = df_fe['smoke'] / df_fe['temperature']\n",
    "    \n",
    "    # Rapid Temperature Rise Indicator\n",
    "    temp_std = df_fe['temperature_rolling_std_30m'].replace(0, 0.1)  # Avoid division by zero\n",
    "    df_fe['rapid_temp_rise'] = df_fe['temperature_diff_30m'] / temp_std\n",
    "    \n",
    "    # Fire Danger Index\n",
    "    df_fe['fire_danger_index'] = (\n",
    "        df_fe['temperature'] * 0.3 + \n",
    "        (100 - df_fe['humidity']) * 0.3 + \n",
    "        df_fe['smoke'] / 1000 * 0.2 + \n",
    "        df_fe['ir_temperature'] * 0.2\n",
    "    )\n",
    "    \n",
    "    # Now generate predictions for each target sensor\n",
    "    model_dir = 'models'\n",
    "    predictions = {}\n",
    "    all_predictions_df = pd.DataFrame()\n",
    "    sequence_length = 60\n",
    "    \n",
    "    # Generate future timestamps\n",
    "    last_timestamp = data['timestamp'].iloc[-1]\n",
    "    future_timestamps = [last_timestamp + timedelta(minutes=i+1) for i in range(steps)]\n",
    "    all_predictions_df['timestamp'] = future_timestamps\n",
    "    \n",
    "    # Make predictions for each target\n",
    "    for target in numerical_columns:\n",
    "        print(f\"Generating predictions for {target}...\")\n",
    "        \n",
    "        try:\n",
    "            # Try to load the optimized model first\n",
    "            model_path = os.path.join(model_dir, f'optimized_lstm_{target}_model.h5')\n",
    "            metadata_path = os.path.join(model_dir, f'fe_lstm_{target}_metadata.pkl')\n",
    "            \n",
    "            if not os.path.exists(model_path):\n",
    "                # Fall back to feature-engineered model\n",
    "                model_path = os.path.join(model_dir, f'fe_lstm_{target}_model.h5')\n",
    "                \n",
    "            # Load model and metadata\n",
    "            model = load_model(model_path)\n",
    "            with open(metadata_path, 'rb') as f:\n",
    "                metadata = pickle.load(f)\n",
    "                \n",
    "            # Extract feature columns and scalers\n",
    "            feature_cols = metadata['feature_cols']\n",
    "            scaler_x = metadata['scaler_x']\n",
    "            scaler_y = metadata['scaler_y']\n",
    "            \n",
    "            # Prepare data for prediction\n",
    "            input_features = df_fe[['timestamp'] + feature_cols + [target]].copy()\n",
    "            \n",
    "            # Handle missing columns (if any)\n",
    "            missing_cols = set(feature_cols) - set(input_features.columns)\n",
    "            if missing_cols:\n",
    "                print(f\"Warning: Missing columns for {target}: {missing_cols}\")\n",
    "                for col in missing_cols:\n",
    "                    input_features[col] = 0.0  # Fill with zeros as fallback\n",
    "                    \n",
    "            # Set index and ensure we have the right columns\n",
    "            input_features = input_features[['timestamp'] + feature_cols + [target]]\n",
    "            input_features.set_index('timestamp', inplace=True)\n",
    "            \n",
    "            # Scale the features\n",
    "            scaled_features = input_features.copy()\n",
    "            try:\n",
    "                scaled_features[feature_cols] = scaler_x.transform(input_features[feature_cols])\n",
    "                scaled_features[[target]] = scaler_y.transform(input_features[[target]])\n",
    "            except Exception as e:\n",
    "                print(f\"Scaling error: {e}\")\n",
    "                print(\"Trying alternative scaling approach...\")\n",
    "                # Alternative approach if direct transform fails\n",
    "                for col in feature_cols:\n",
    "                    scaled_features[col] = (input_features[col] - input_features[col].min()) / \\\n",
    "                                          (input_features[col].max() - input_features[col].min() + 1e-10)\n",
    "                scaled_features[target] = (input_features[target] - input_features[target].min()) / \\\n",
    "                                         (input_features[target].max() - input_features[target].min() + 1e-10)\n",
    "                \n",
    "            # Get the last sequence\n",
    "            last_sequence = scaled_features.iloc[-sequence_length:].values\n",
    "            \n",
    "            # Generate future predictions\n",
    "            future_predictions = []\n",
    "            current_sequence = last_sequence.copy()\n",
    "            \n",
    "            for _ in range(steps):\n",
    "                # Reshape for prediction\n",
    "                current_batch = current_sequence.reshape(1, current_sequence.shape[0], current_sequence.shape[1])\n",
    "                \n",
    "                # Predict next value\n",
    "                next_pred = model.predict(current_batch, verbose=0)[0][0]\n",
    "                future_predictions.append(next_pred)\n",
    "                \n",
    "                # Update the sequence - create a copy of the last step\n",
    "                new_step = current_sequence[-1].copy()\n",
    "                # Find the position of the target column in the features\n",
    "                target_idx = len(feature_cols)  # Target should be the last column\n",
    "                new_step[target_idx] = next_pred  # Update the target value\n",
    "                \n",
    "                # Roll the sequence - remove first, add new step at the end\n",
    "                current_sequence = np.vstack([current_sequence[1:], [new_step]])\n",
    "                \n",
    "            # Convert scaled predictions back to original scale\n",
    "            future_predictions = np.array(future_predictions).reshape(-1, 1)\n",
    "            \n",
    "            try:\n",
    "                # Use scaler inverse transform if possible\n",
    "                original_scale_predictions = scaler_y.inverse_transform(future_predictions).flatten()\n",
    "            except Exception as e:\n",
    "                print(f\"Inverse scaling error: {e}\")\n",
    "                # Fallback to manual inverse scaling\n",
    "                min_val = input_features[target].min()\n",
    "                max_val = input_features[target].max()\n",
    "                original_scale_predictions = future_predictions.flatten() * (max_val - min_val) + min_val\n",
    "                \n",
    "            # Store predictions\n",
    "            predictions[target] = original_scale_predictions\n",
    "            all_predictions_df[target] = original_scale_predictions\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error predicting {target}: {e}\")\n",
    "            # If prediction fails, fill with the last known value\n",
    "            last_value = data[target].iloc[-1]\n",
    "            predictions[target] = np.array([last_value] * steps)\n",
    "            all_predictions_df[target] = [last_value] * steps\n",
    "    \n",
    "    # Calculate Fire Danger Index for predictions\n",
    "    all_predictions_df['fire_danger_index'] = (\n",
    "        all_predictions_df['temperature'] * 0.3 + \n",
    "        (100 - all_predictions_df['humidity']) * 0.3 + \n",
    "        all_predictions_df['smoke'] / 1000 * 0.2 + \n",
    "        all_predictions_df['ir_temperature'] * 0.2\n",
    "    )\n",
    "    \n",
    "    # Save both historical and predicted data to CSV if requested\n",
    "    if save_results:\n",
    "        # Prepare historical data with the same structure as predictions\n",
    "        historical_data = data.copy().reset_index(drop=True)\n",
    "        \n",
    "        # Calculate Fire Danger Index for historical data too\n",
    "        historical_data['fire_danger_index'] = (\n",
    "            historical_data['temperature'] * 0.3 + \n",
    "            (100 - historical_data['humidity']) * 0.3 + \n",
    "            historical_data['smoke'] / 1000 * 0.2 + \n",
    "            historical_data['ir_temperature'] * 0.2\n",
    "        )\n",
    "        \n",
    "        # Select only relevant columns to match predictions\n",
    "        columns_to_keep = ['timestamp', 'temperature', 'humidity', 'smoke', 'ir_temperature', 'fire_danger_index']\n",
    "        historical_data = historical_data[columns_to_keep]\n",
    "        \n",
    "        # Add a column to identify data type (historical vs predicted)\n",
    "        historical_data['data_type'] = 'historical'\n",
    "        all_predictions_df['data_type'] = 'predicted'\n",
    "        \n",
    "        # Combine historical and predicted data\n",
    "        combined_df = pd.concat([historical_data, all_predictions_df], ignore_index=True)\n",
    "        \n",
    "        # Save to CSV\n",
    "        timestamp_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        csv_filename = f\"combined_data_{timestamp_str}.csv\"\n",
    "        combined_df.to_csv(csv_filename, index=False)\n",
    "        print(f\"Combined historical and predicted data saved to {csv_filename}\")\n",
    "        \n",
    "        # Also save just the predictions if needed\n",
    "        pred_csv_filename = f\"predictions_{timestamp_str}.csv\"\n",
    "        all_predictions_df.to_csv(pred_csv_filename, index=False)\n",
    "        print(f\"Predictions only saved to {pred_csv_filename}\")\n",
    "        \n",
    "    return all_predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of the prediction function\n",
    "# To use with your own data, replace this with your dataframe containing at least 60 timesteps\n",
    "if 'df4' in globals() and len(df4) >= 460:\n",
    "    print(\"Making predictions using rows 400 to 460 from the cleaned dataset...\")\n",
    "    sample_data = df4.iloc[400:460].reset_index()\n",
    "    future_predictions = make_future_predictions(sample_data, steps=24, save_results=True)\n",
    "    print(\"\\nPrediction Results:\")\n",
    "    display(future_predictions)\n",
    "else:\n",
    "    print(\"No suitable dataset available for prediction. Please provide a dataframe with at least 460 records.\")\n",
    "    print(\"Example usage:\")\n",
    "    print(\"future_predictions = make_future_predictions(your_data.iloc[400:460], steps=24)\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "research-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
